# 赛道设置与评估
## 说话人相关的语音识别
说话人相关的ASR任务需要从重叠的语音中识别每个说话人的语音，并为识别内容分配一个说话人标签。图2展示了说话人相关语音识别任务和多说话人语音识别任务的主要区别。在本次竞赛中AliMeeting、Aishell4和Cn-Celeb数据集可作为受限数据源。在M2MeT挑战赛中使用的AliMeeting数据集包含训练、评估和测试集，在M2MeT2.0可以在训练和评估中使用。此外，一个包含约10小时会议数据的新的Test-2023集将根据赛程安排发布并用于挑战赛的评分和排名。值得注意的是，对于Test-2023测试集，主办方将不再提供耳机的近场音频、转录以及真实时间戳。而是提供可以通过一个简单的VAD模型得到的包含多个说话人的片段。

![task difference](images/task_diff.png)

## 评估方法
使用串联最优排序字符错误率（cpCER）指标来评估说话人相关的ASR系统的准确性。cpCER的计算包括三个步骤。首先，将一场会议中每个说话人的参考和假设转录按时间顺序串联起来。其次，计算真实标签和预测输出之间的字符错误率（CER），并对所有可能的说话人排列组合重复这一过程。最后，选择CER最低的排列组合作为该时段的cpCER。CER是通过将ASR输出转化为参考抄本所需的插入（Ins）、替换（Sub）和删除（Del）的字符总数除以参考抄本的字符总数得到的。具体来说，CER的计算方法是：

$$ \text{CER} = \frac {\mathcal N_{\text{Ins}} + \mathcal N_{\text{Sub}} + \mathcal N_{\text{Del}} }{\mathcal N_{\text{Total}}} \times 100\%, $$

其中 $\mathcal N_{\text{Ins}}$ , $\mathcal N_{\text{Sub}}$ , $\mathcal N_{\text{Del}}$ 是三种错误的字符数, $\mathcal N_{\text{Total}}$ 是字符总数.
## 子赛道设置
### 子赛道一 (限定训练数据):
参赛者在系统构建过程中仅能使用AliMeeting、AISHELL-4和CN-Celeb，严禁使用额外数据。参赛者可以任何第三方开源的预训练模型，如[Hugging Face](https://huggingface.co/models)以及[ModelScope](https://www.modelscope.cn/models)上提供的模型。参赛者需要在最终的系统描述文档中详细列出使用的预训练模型名称以及链接。
### 子赛道二 (开放训练数据):
除了限定数据外，参与者可以使用任何公开可用、私人录制和模拟仿真的数据集。但是，参与者必须清楚地列出使用的数据。同样，参赛者也可以使用任何第三方开源的预训练模型，但必须在最后的系统描述文件中明确的列出所使用的数据和模型链接，如果使用模拟仿真数据，请详细描述数据模拟的方案。